{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d4316-b1ef-4b03-95b8-4c2ea6b4b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "\n",
    "url = 'https://www.baraasallout.com/test.html?authuser=1'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "def extract_text_data():\n",
    "    headings = []\n",
    "    paragraphs = []\n",
    "    list_items = []\n",
    "\n",
    "    for heading in soup.find_all(['h1', 'h2']):\n",
    "        headings.append(heading.get_text())\n",
    "\n",
    "    for p in soup.find_all('p'):\n",
    "        paragraphs.append(p.get_text())\n",
    "\n",
    "    for li in soup.find_all('li'):\n",
    "        list_items.append(li.get_text())\n",
    "\n",
    "    with open('Extract_Text_Data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Headings', 'Paragraphs', 'List Items'])\n",
    "        for i in range(max(len(headings), len(paragraphs), len(list_items))):\n",
    "            writer.writerow([headings[i] if i < len(headings) else '',\n",
    "                             paragraphs[i] if i < len(paragraphs) else '',\n",
    "                             list_items[i] if i < len(list_items) else ''])\n",
    "def extract_table_data():\n",
    "    table_data = []\n",
    "\n",
    "    table = soup.find('table')\n",
    "    if table:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows[1:]: \n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 2: \n",
    "                product_name = cols[0].get_text()\n",
    "                price = cols[1].get_text()\n",
    "                stock_status = cols[2].get_text()\n",
    "                table_data.append([product_name, price, stock_status])\n",
    "\n",
    "        with open('Extract_Table_Data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Product Name', 'Price', 'Stock Status'])\n",
    "            writer.writerows(table_data)\n",
    "\n",
    "def extract_product_info():\n",
    "    product_data = []\n",
    "\n",
    "    cards = soup.find_all('div', class_='product-card') \n",
    "    for card in cards:\n",
    "        title = card.find('h3').get_text()\n",
    "        price = card.find('span', class_='price').get_text() \n",
    "        stock = card.find('span', class_='stock').get_text() if card.find('span', class_='stock') else 'N/A' \n",
    "        button_text = card.find('button').get_text() if card.find('button') else 'No button' \n",
    "        product_data.append({\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'stock': stock,\n",
    "            'button_text': button_text\n",
    "        })\n",
    "    with open('Product_Information.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(product_data, file, ensure_ascii=False, indent=4)\n",
    "def extract_form_details():\n",
    "    form_data = []\n",
    "\n",
    "    forms = soup.find_all('form')\n",
    "    for form in forms:\n",
    "        fields = form.find_all(['input', 'textarea', 'select'])\n",
    "        for field in fields:\n",
    "            field_name = field.get('name')\n",
    "            input_type = field.get('type')\n",
    "            default_value = field.get('value', 'No default value')\n",
    "            form_data.append({\n",
    "                'field_name': field_name,\n",
    "                'input_type': input_type,\n",
    "                'default_value': default_value\n",
    "            })\n",
    "    with open('Form_Details.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(form_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def extract_links_and_multimedia():\n",
    "    link_data = []\n",
    "\n",
    "    links = soup.find_all('a', href=True)\n",
    "    for link in links:\n",
    "        link_data.append({'url': link['href']})\n",
    "\n",
    "    iframes = soup.find_all('iframe', src=True)\n",
    "    for iframe in iframes:\n",
    "        link_data.append({'video_url': iframe['src']})\n",
    "\n",
    "    with open('Links_and_Multimedia.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(link_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def extract_featured_products():\n",
    "    featured_products = []\n",
    "\n",
    "    products = soup.find_all('span', class_='name')\n",
    "    for product in products:\n",
    "        name = product.get_text()\n",
    "        price = product.find_next('span', class_='price').get_text() \n",
    "        colors = product.find_next('span', class_='colors').get_text() \n",
    "        product_id = product.get('data-id') \n",
    "        featured_products.append({\n",
    "            'id': product_id,\n",
    "            'name': name,\n",
    "            'price': price,\n",
    "            'colors': colors\n",
    "        })\n",
    "    with open('Featured_Products.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(featured_products, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "extract_text_data()\n",
    "extract_table_data()\n",
    "extract_product_info()\n",
    "extract_form_details()\n",
    "extract_links_and_multimedia()\n",
    "extract_featured_products()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
